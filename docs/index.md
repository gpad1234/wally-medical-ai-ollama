---
layout: default
title: Home
---

# WALLY â€” Medical AI Reasoner

**Symptom-based diagnostic reasoning powered by a local ontology knowledge graph + Ollama LLM**

[![Python](https://img.shields.io/badge/Python-3.12-blue?style=for-the-badge&logo=python)](https://python.org)
[![Ollama](https://img.shields.io/badge/LLM-llama3.2%3A3b-orange?style=for-the-badge)](https://ollama.com)
[![GitHub](https://img.shields.io/badge/GitHub-wally--medical--ai--ollama-black?style=for-the-badge&logo=github)](https://github.com/gpad1234/wally-medical-ai-ollama)

---

## What is WALLY?

WALLY is a local research project that wires an **RDF/OWL medical ontology** directly to a **locally-running LLM** (Ollama `llama3.2:3b`) through a React UI.

Select symptoms â†’ **two reasoning engines run side-by-side**:

| Engine | How it works | Speed |
|--------|-------------|-------|
| ğŸ” **JS Ontology Reasoner** | Weighted graph traversal scores diseases against symptoms | Instant |
| ğŸ¦™ **Ask AI (Ollama)** | LLM prompted with ontology context gives natural-language reasoning | ~5â€“15s |

**100% local** â€” no cloud, no API keys, $0 cost.

---

## âœ¨ Key Features

- **Medical knowledge graph** â€” `medical_ontology.ttl` with 7 diseases, 20 symptoms, 14 treatments (RDF/OWL)
- **Ontology-grounded prompting** â€” `/api/diagnose` builds a structured prompt from live TTL data then calls the LLM
- **Side-by-side comparison** â€” JS reasoner + LLM response shown together
- **Confidence scoring** â€” percentage match with ontology classification chain
- **97 pytest tests** â€” full unit coverage of service and adapter layers

---

## ğŸ¬ Quick Start

```bash
# 1 â€” environment
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
cd src/core && make && cd ../..

# 2 â€” Ollama
ollama pull llama3.2:3b && ollama serve

# 3 â€” Flask API
PYTHONPATH=$(pwd) python graph/ontology_api.py

# 4 â€” React UI
cd graph-ui && npm install && npm run dev
```

Open **[http://localhost:5173](http://localhost:5173)** â†’ pick symptoms â†’ hit **ğŸ¦™ Ask AI (Ollama)**.

[ğŸ“– Full Getting Started Guide â†’](getting-started)

---

## ğŸ—ï¸ Architecture

```
Browser (React 18 + Vite :5173)
        â”‚
        â”œâ”€ GET  /api/ontology/medical  â”€â”€â–º Flask :5002
        â”‚                                      â”‚
        â””â”€ POST /api/diagnose  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                               â”‚  rdflib parses medical_ontology.ttl
                                               â”‚  builds ontology-grounded prompt
                                               â–¼
                                        Ollama :11434
                                        llama3.2:3b (local)
```

---

## ğŸ“¦ Tech Stack

| Layer | Technology |
|-------|-----------|
| Frontend | React 18 + Vite |
| Backend API | Flask + Python 3.12 |
| Ontology parsing | rdflib 7.6.0 |
| LLM runtime | Ollama (local) |
| LLM model | llama3.2:3b |
| C core library | libsimpledb â€” FNV-1a hash table (ctypes) |
| Testing | pytest 8 + pytest-cov |

---

<div style="text-align: center; padding: 40px 0; background: #f8fafc; margin-top: 40px; border-radius: 8px;">
<p style="font-size: 18px; margin-bottom: 15px;">Ready to run WALLY locally?</p>
<a href="getting-started" style="display: inline-block; background: #3b82f6; color: white; padding: 12px 30px; border-radius: 6px; text-decoration: none; font-weight: bold;">Get Started â†’</a>
&nbsp;&nbsp;
<a href="api" style="display: inline-block; background: #10b981; color: white; padding: 12px 30px; border-radius: 6px; text-decoration: none; font-weight: bold;">API Reference â†’</a>
</div>
